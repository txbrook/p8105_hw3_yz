---
title: "p8105_hw3_yz4437.Rmd"
author: "Yuchen Zhang"
date: "2022-10-14"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)


knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

```{r }
library(p8105.datasets)
#### Read in the data
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

#### Answer questions about the data

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns, with each row resprenting a single product from an instacart order. Variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are several order-level variables, describing the day and time of the order, and number of days since prior order. Then there are several item-specific variables, describing the product name (e.g. Yogurt, Avocado), department (e.g. dairy and eggs, produce), and aisle (e.g. yogurt, fresh fruits), and whether the item has been ordered by this user in the past. In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.

Below is a table summarizing the number of items ordered from aisle. In total, there are 134 aisles, with fresh vegetables and fresh fruits holding the most items ordered by far.

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Next is a plot that shows the number of items ordered in each aisle. Here, aisles are ordered by ascending number of items.

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```

Our next table shows the three most popular items in aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`, and includes the number of times each item is ordered in your table.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

Finally is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. This table has been formatted in an untidy manner for human readers. Pink Lady Apples are generally purchased slightly earlier in the day than Coffee Ice Cream, with the exception of day 5.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

## Problem 2

### tidy data

clean the variables' names, a variable `day_type` is created for specifies weekday or weekend, convert `day_type` and `day` into factor variables. pivot long and create a variable `activity` for acitivtity number and `activity_count` for activity count.

```{r}
accel = read_csv("data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
  activity_1:activity_1440,
  names_to = "activity", 
  names_prefix = "activity_",
  values_to = "activity_counts"
  ) %>% 
  mutate(
    day_type = ifelse(day == "Sunday" | day == "Saturday","weekend","weekday"),
    day_type = as.factor(day_type),
    day = as.factor(day),
    activity = as.numeric(activity)
  ) 
accel
```

The resulting dataset has `r ncol(accel)` variables, `week`: week number, `day_id`: day number, `day`: day of week, `day_type`:weekday or weekend, `activity_num`: the minute of the day and `activity_count`: the activity count of associated minute on the associated day. Dataset has `r nrow(accel)` observations for acitivity counts of specific minute and day.

### analysis

a variable `activity_sum` is created for sum of activity counts of associated day and a table is created using kable.

```{r}
  accel %>% 
    group_by(day_id) %>% 
    summarize(
      day_sum = sum(activity_counts),
    ) %>% 
  knitr::kable(col.names = c("Day id","Total activity counts"),digit = 2)
```

There is not significant trend from table.

### plot

a single-panel plot that shows the 24-hour activity time courses for each day and color for indicate day of the week.

```{r}
accel %>%  
  ggplot(aes(x = activity, y = activity_counts, color = day)) +
  geom_point() +
  geom_line() +
  labs(
    title = "24-hour activity time courses",
    x = "Minutes of day(minute)",
    y = "Activity counts"
  ) +
    scale_x_continuous(
    breaks = seq(0,1440,120), 
    labels = seq(0,1440,120)
    ) +
  scale_color_discrete(name = "Day of the week")
```

According to this graph, activity counts during the day time is mostly greater than that at night, and the activity counts on weekdays is mostly greater than that on weekends. Activity counts mostly peak at 20:00 to 22:00.

## Problem 3

load the data
```{r}
library(p8105.datasets)
data("ny_noaa")
```

The size of the dataset is `r dim(ny_noaa)[1]` x `r dim(ny_noaa)[2]`, the structure of the data is `r nrow(ny_noaa)` observations have `r ncol(ny_noaa)` variables. Some key variables are `id` :Weather station ID, `date` :Date of observation, `prcp` :Precipitation (tenths of mm), `snow` :Snowfall (mm), `snwd` :Snow depth (mm), `tmax` :Maximum temperature (tenths of degrees C) and `tmin` :Minimum temperature (tenths of degrees C).

There are `r round(sum(is.na(ny_noaa$prcp))/nrow(ny_noaa),2)*100`% missing data for `prcp`, `r round(sum(is.na(ny_noaa$snow))/nrow(ny_noaa),2)*100`% missing data for `snow`, `r round(sum(is.na(ny_noaa$snwd))/nrow(ny_noaa),2)*100`% missing data for `snwd`, `r round(sum(is.na(ny_noaa$tmax))/nrow(ny_noaa),2)*100`% missing data for `tmax`, `r round(sum(is.na(ny_noaa$tmin))/nrow(ny_noaa),2)*100`% missing data for `tmin`. Missing data accounts for a large portion of dataset. But, it has little effect on the macro results.

### clean data 

separate `date` variable into `year`, `month`, and `day`, convert`tmin` and `tmax` to numberic variable and handle observations for temperature, precipitation, and snowfall to reasonable units.

```{r}
ny_noaa_tidy = 
  mutate(ny_noaa,
    year = as.integer(lubridate::year(date)), 
    month = as.integer(lubridate::month(date)), 
    day = as.integer(lubridate::day(date)),
    month = month.abb[month],
    prcp = prcp/10,
    tmin = as.numeric(tmin)/10,
    tmax = as.numeric(tmax)/10
    )
```

Count the most commonly observed values for snowball.

```{r}
ny_noaa %>%
  group_by(snow) %>%
  summarise(
    n_obs = n()
    ) %>%
  arrange(-n_obs) %>%
  head(3)
```

The most commonly observed values for snowfall is 0 mm, becasue 0 appears most frequently.

### average max temperature in January and in July

calculate the average max temperature in January and in July in each station each year, and make a two-panel plot for the average `tmax` across years.

```{r}
ny_noaa_tidy %>% 
  filter(month %in% c("Jan","Jul")) %>% 
  group_by(id,year,month) %>% 
  mutate(
    avg_tmax = mean(tmax, na.rm = TRUE)
    ) %>% 
  
  ggplot(aes(x = year, y = avg_tmax, color = id)) + 
  geom_line(alpha = 0.5)  +
  facet_grid(.~month) +
  theme(legend.position = "none") +
  labs(
    title = "Average maximum temperature in Jan and Jul from each station",
    x = "Year",
    y = "Average maximum daily temperature (C)",
    caption = "Data from NOAA"
    ) 

```

According to plot,The average max temperature in January is about 0C lower than July which is about 27C. There is no significant change across years.

create thresholds can find the outliers for average max temperature in January and July by filter data

```{r}
outliers = 
  ny_noaa_tidy %>% 
  filter(month %in% c("Jan","Jul")) %>% 
  group_by(id,year,month) %>% 
  mutate(
    avg_tmax = mean(tmax, na.rm = TRUE)
  ) %>% 
  group_by(month) %>% 
  mutate(
    threshold_min = median(avg_tmax, na.rm = TRUE) - (3 * mad(avg_tmax, na.rm = TRUE)),
    threshold_max = median(avg_tmax, na.rm = TRUE) + (3 * mad(avg_tmax, na.rm = TRUE))
  ) %>% 
  filter(avg_tmax < threshold_min | avg_tmax > threshold_max) %>% 
  select(id,year,month,avg_tmax) %>% 
  distinct()
outliers
```

there are several outliers of average max temperature in Jan and Jul according to this threshold.

### tmax_tmin & snowfall 

hexagonal heatmap of tmax vs tmin for the full dataset and boxplot for distribution of snowfall values greater than 0 and less than 100 separately by year
```{r}
tmax_tmin =
  ny_noaa_tidy %>% 
  ggplot(aes(x = tmax, y = tmin)) + 
  geom_hex()+
  theme(
        legend.key.width = unit(2,"cm")
  )+
  labs(
    title = "Max temperature vs Min temperature",
    x = "Minimum temperature (C)",
    y = "Maximum temperature (C)",
    caption = "Data from NOAA"
  )

snowfall = 
  ny_noaa_tidy %>%
  filter(snow>0,snow<100) %>%
  ggplot(aes(x = snow, y = as.factor(year), fill = year)) + 
  geom_density_ridges(scale = 1)+
  labs(
    title = "Distribution of snowfall in each year",
    y = "Year",
    x = "Snow fall (mm)",
    caption = "Data from NOAA"
  )

tmax_tmin + snowfall
```

According to plot tmax vs tmin, the maximum temperature focus around 15C and the minimum temperature is focus around 5C.Maximum and minimum temperatures are roughly proportional.

The distribution of snowfall values greater than 0 and less than 100 separately by year are most between 0 mm and 30 mm and at peak in 25 mm.
